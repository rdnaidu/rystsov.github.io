---
layout: post
title: How Paxos works
name: How Paxos works
tags: ["pre_distr"]
desc: "A memo on how Paxos works"
has_comments: true
has_math: true
ignore_css: true
marker: hpw
---

<div class="abstract">
	<h1>How Paxos works</h1>
	<p>
		<span class="label">Abstract.</span>
		<span class="text">One day I understood Paxos, but a couple of months later I realized that I actually didn't. I reread 'Paxos Made Simple' but it was almost as hard as the first time so I wrote this memo to help myself in the future to get Paxos faster.</span>
	</p>
</div>

<div id="overview1" class="brim"><div class="content">
	<h2>Overview</h2>
</div></div>

<div class="brim"><div class="content">
	<p class="edge">Paxos is a class of synod-based algorithms for building available consistent distributed systems on top of asynchronous and unreliable network. For example, if you're building a key/value storage, Paxos will help you to keep it working in the presence of network errors (partition-tolerance) and node failures (availability), and will help you produce non-contradictory view to different clients (consistency).</p>
	
	<blockquote>&gt; Doesn't it violate CAP theorem?</blockquote>
		
		<p>No, it doesn't. Availability in the <a class="link" href="https://en.wikipedia.org/wiki/CAP_theorem">CAP</a>-sense is very strict. For example, a system using two-phase commit (2PC) algorithm and a system using Paxos algorithm are both unavailable in a CAP sense. It makes sense for the 2PC system since the 2PC's coordinator is a single point of failure but it's strange for Paxos because it tolerates up to N fails out of 2N+1 nodes.</p>

		<p>So Paxos is an available CP system.</p>
	
	<blockquote>&gt; What can we build with Paxos?</blockquote>
		
		<img src="{{ site.url }}/images/put-on-paxos.jpg" width="500"/>

		<p>We can build a distributed state machine and implement any algorithm on top of it. But it's very hard to think about an unbounded domain, so in this post we consider Paxos as a foundation for building distributed data storages.</p>

		<p>It's a common practice for storages to have write operations to mutate its state and read operations to query it. Paxos is different, it guarantees consistency only for write operations, so to query its state the system makes read, writes the state back and when the state change is accepted the system queries the written state.</p>

	<blockquote>&gt; Wait, don't pay so much attention to the details, what is the topology of the Paxos-based distributed system?</blockquote>

		<p>Usually a Paxos-based distributed system consists of clients, proposers and acceptors. Clients mutate and query the state of the system, proposers process the client's commands and acceptors store information.</p>

		<p>The Paxos topology is similar to the typical 3-tier application where clients correspond to web-browsers, proposers to web servers and acceptors to databases.</p>

	<blockquote>&gt; If proposers are similar to front-end servers does it mean that the proposers are stateless?</blockquote>

		<p>No, each of the proposers should be able to generate a sequence of increasing ID (ballot numbers) which doesn't intersect with the sequences of other proposers. So proposers must have state to store the last used ballot number. There're multiple ways how to generate such sequences with and without coordination.</p>
		
		<p>For example each server may have unique ID and use an increasing sequence of natural number n to generate (n,ID) tuples and use tuples as ballot numbers. To compare them we start by comparing the first element from each tuple. If they are equal, we use the second component of the tuple (ID) as a tie breaker.</p>
		
		<p>Let IDs of two servers are 0 and 1 then two sequences they generate are (0,0),(1,0),(2,0),(3,0).. and (0,1),(1,1),(2,1),(3,1).. Obviously they are unique, ordered and for any element in one we always can peak an greater element from another.</p>

	<blockquote>&gt; Ok, how do proposers and acceptors communicate to agree on the system state?</blockquote>

		<p>Let's take a look on how a Paxos-based distributed system handles a state mutation request. The typical case is:</p>

		<ol>
			<li>Client connects to any proposer and issues the command.</li>
			<li>The proposer commnicates with the acceptors and they agree on the system's state.</li>
			<li>Once the change is accepted all future reads should reflect the change.</li>
		</ol>

		<img src="{{ site.url }}/images/paxos-seq.png" width="600"/>

		<p>On the diagram we see two rounds of proposer-acceptors communications. We also can estimate that the system generates from <code>4f+6</code> to <code>8f+6</code> messages for every change/read where <code>f</code> is a number of failures that the system can tolerate.</p>

		<p>If something bad happens and client doesn't receive a confirmation then she should query the system to understand if her change was applied or not. For example it may happen when the concurrent requests from the clients collide and abort each other.</p>
</div></div>

<div id="code1" class="brim"><div class="content">
	<h2>Code</h2>
</div></div>

<div class="brim"><div class="content">
	<p class="edge">Ok. Let's start with the acceptor's code. As you can see from the sequence diagram it should support two phases: preparation and acceptance. They are supported via the corresponding endpoints. The algorithm itself is written in a Python inspired pseudocode. I assume that every hdd.write call is flushed to the disk.</p>

		{% gist rystsov/44b25528e74bb617726d %}

		<p>As you remember, clients communicate only with the proposers so it's a good idea to explore its API. I did mention before that Paxos guarantees consistency only for write requests so it shouldn't be a surprise that the proposer's API consists just of one <code>change_query</code> endpoint.</p>

		<p>It accepts two pure functions: <code>change</code> and <code>query</code>. The <code>change</code> function validates the old state and maps it into the new state (or throws an exception). The <code>query</code> function makes a projection of the new state and returns it to the client.</p>

		<p>Consider that we want to work with a distributed variable, then we may use identical transformation both as <code>change</code> and as <code>query</code>. If we want to perform a CAS-guarded state transition from old to new value then we should use the following change-generator:</p>

		{% gist rystsov/1517327d88eb3576ef94 %}

		<p>Once we digest all the previous information the proposal's source code shouldn't be too scary.</p>

		{% gist rystsov/ca9d195b2737039faaf3 %}
</div></div>

<div id="math1" class="brim"><div class="content">
	<h2>Math</h2>
</div></div>

<div class="brim"><div class="content">
	<p class="edge">The Paxos algorithm gives us a way to build reliable distributed data structures which keep working in a predictable way even if the whole system experiences network partitioning or node failures. As long as a client can communicate with a proposer and the proposer sees the quorum of the acceptors then the distributed storage behaves like a thread safe data structure. Otherwise the system is unavailable.</p>
</div></div>


<div class="brim alt defs1"><div class="content">
	<p class="edge">Let's prove it. As you can see from the sources the system generates events of different kind (emit_executed, emit_prepared, emit_accepted and emit_promised). We will show that:</p>

		<ol>
			<li>there is a <a class="link" href="https://en.wikipedia.org/wiki/Binary_relation">relation</a> between emit_accepted events which captures causality between them</li>
			<li>the relation is also defined on emit_executed</li>
			<li>the reduction of the relation on emit_executed is <a class="link" href="https://en.wikipedia.org/wiki/Total_order">total order</a></li>
		</ol>

		<p>The 3rd bullet means that any successful change is an effect of the previously successful change.</p>

		<p>We should introduce a couple of contractions for the events name to simplify the reasoning about the space of events.</p>

		<table>
		    <tr><th>In the code</th><th>In the proof</th></tr>
                    <tr><td>emit_executed(n,...)</td><td class="abbr">$\bar{n}^2$</td></tr>
		    <tr><td>emit_prepared(n,...)</td><td class="abbr">$\bar{n}^1$</td></tr>
		    <tr><td>emit_accepted(n,...)</td><td class="abbr">$\ddot{n}^2$</td></tr>
		    <tr><td>emit_promised(n,...)</td><td class="abbr">$\ddot{n}^1$</td></tr>
		</table>

		<p>$\bar{n}^1$ and $\bar{n}^2$ are single events, but $\ddot{n}^1$ and $\ddot{n}^2$ are sets of events where each event corresponds to an acceptor.</p>

		<p>By the definition:</p>

		$$ \forall \ddot{b}^2 \in \mathrm{E} \quad \exists \ddot{a}^2 \in \mathrm{E} \,:\, \mathrm{s}(\ddot{b}^2) = \mathrm{change}(\mathrm{s}(\ddot{a}^2))$$

		<p>where <span class="imb">$\mathrm{E}$</span> is a space of events generated by the system and <span class="imb">$\mathrm{s}(x) := x.\mathrm{state}$</span>. In such cases we say that $\ddot{a}^2$ is an ancestor of $\ddot{b}^2$ and we use the subset sign to express it: <span class="iml">$\mathrm{s}(\ddot{a}^2) \subset \mathrm{s}(\ddot{b}^2)$</span>.</p>
		
		<p>Let's extend $\subset$ to be transitive: we say that <span class="imb">$\mathrm{s}(\ddot{a}^2) \subset \mathrm{s}(\ddot{c}^2)$</span> if <span class="imb">$\mathrm{s}(\ddot{a}^2) \subset \mathrm{s}(\ddot{b}^2)$</span> and <span class="imb">$\mathrm{s}(\ddot{b}^2) \subset \mathrm{s}(\ddot{c}^2)$</span>. Since causality is also transitive this extension respects it.</p>

		<p>We want to extend the $\subset$ relation on $\bar{x}^2$ events. We say that <span class="imb">$\mathrm{s}(\bar{x}^2) \subset y$</span> if <span class="iml">$\forall \ddot{x}^2 \; \mathrm{s}(\ddot{x}^2) \subset y$</span>. For example</p>
		
		$$\mathrm{s}(\bar{x}^2) \subset \mathrm{s}(\bar{y}^2)$$

		<p>means</p>

		$$\forall \ddot{x}^2 \; \forall \ddot{y}^2 \,:\, \mathrm{s}(\ddot{x}^2) \subset \mathrm{s}(\ddot{y}^2)$$
</div></div>


<div class="brim theorem1"><div class="content theorem">
	<div class="label">Statement.</div>
		<div class="text">
			<p>We're proving that:</p>

			$$\mathrm{s}(\bar{n}^2) \subset \mathrm{s}(\bar{m}^2) \;\vee\; \mathrm{s}(\bar{m}^2) \subset \mathrm{s}(\bar{n}^2)$$

			<p>It obviously holds for any track (a set of events generated by the system) if the following expression is true for the same track:</p>

			$$n&lt;m \;\Rightarrow\; \mathrm{s}(\bar{n}^2) \subset \mathrm{s}(\bar{m}^2)$$

			<p>Which holds if:</p>

			$$n&lt;m \;\Rightarrow\; \mathrm{s}(\bar{n}^2) \subset \mathrm{s}(\ddot{m}^2)$$

			<p>Let's do it.</p>
		</div>
	<div class="label">Proof.</div>
		<div class="text">
			<p>First we define <code>unwrap</code> function which maps a ballot number of the write to the ballot number of the previous write.</p>

			$$\mathrm{unwrap}(\ddot{x}^2) := \bar{x}^1.\mathrm{reads}.\mathrm{max}(x \to x.\mathrm{accepted\_n}).\mathrm{accepted\_n}$$

			<p>We may think about <code>unwrap</code> as an inversion of the <code>change</code>, because</p>

			$$\mathrm{s}(\ddot{b}^2) = \mathrm{change}(\mathrm{s}(\ddot{a}^2)) \;\Rightarrow\; a=\mathrm{unwrap}(\ddot{b}^2)$$

			<p>We also need a couple of lemmas:</p>

			<ol>
				<li>$\mathrm{s}(\ddot{b}^2) = \mathrm{change}(\mathrm{s}(\ddot{a}^2)) \;\Rightarrow\; a&lt;b$</li>
				<li>$\forall \bar{a}^2 \in \mathrm{E} \quad \forall b>a \,:\, \ddot{b}^2 \in \mathrm{E} \;\Rightarrow\; a \leq \mathrm{unwrap}(\ddot{b}^2)$</li>
			</ol>

			<p>We want to prove the following statement:</p>

			$$n&lt;m \;\Rightarrow\; \mathrm{s}(\bar{n}^2) \subset \mathrm{s}(\ddot{m}^2)$$

			<ol>
				<li><span class="im">$k := 0$</span>,<span class="iml">$z := m$</span></li>
				<li>$z_{k+1} := \mathrm{unwrap}(\ddot{z_k}^2),\;k:=k+1$. Lemma 1 guarantees that for any <span class="imb">$y&lt;x$</span> the <span class="im">$\ddot{z_x}^2$</span> is an ansestor of <span class="im">$\ddot{z_y}^2$</span></li>
				<li>Lemma 2 states that $n \leq z_k$. So we have two cases:
					<ol type="i">
						<li>If <span class="imb">$n &lt; z_k$</span> then goto step #2</li>
						<li>If <span class="imb">$n = z_k$</span> then <span class="iml">$\mathrm{s}(\bar{n}^2) \subset \mathrm{s}(\ddot{m}^2)$</span> because $z_k$ is an ansestor of $z_0$ which is $m$</li>
					</ol>
				</li>
			</ol>

			<div class="qed">Q.E.D.</div>
		</div>
</div></div>

<div class="brim alt lemma1"><div class="content theorem">
	<div class="label">Lemma 1.</div>
		<div class="text"><span class="imr">$\mathrm{s}(\ddot{b}^2) = \mathrm{change}(\mathrm{s}(\ddot{a}^2)) \;\Rightarrow\; a&lt;b$</span> is true because we explicitly check it in the proposer's source code, see <a class="link" href="https://gist.github.com/rystsov/ca9d195b2737039faaf3#file-how-paxos-proposer-py-L24">the monotonicity assert</a>.</div>
</div></div>

<div class="brim lemma2"><div class="content theorem">
	<div class="label">Lemma 2.</div>
		<div class="text">
			$\forall \bar{a}^2 \in \mathrm{E} \quad \forall b>a \,:\, \ddot{b}^2 \in \mathrm{E} \;\Rightarrow\; a \leq \mathrm{unwrap}(\ddot{b}^2)$
		</div>
	<div class="label">Proof.</div>
		<div class="text">
			<p>Since we don't write a new state $\ddot{b}^2$ unless we got a confirmation from the majority $\bar{b}^1$ then the following statement holds:</p>

			$$\forall \ddot{b}^2 \in \mathrm{E} \;\Rightarrow\; \forall \bar{b}^1 \in \mathrm{E}$$
			
			<p>Proposer should receive promises from a majority of the acceptors before it generates a $\ddot{b}^1$ event. It guarantees truth of the following expression:</p>

			$$(\mathrm{N} := \bar{b}^1.\mathrm{reads}.[\mathrm{node\_id}] \cap \bar{a}^2.\mathrm{writes}.[\mathrm{node\_id}]) \neq \emptyset$$

			<p>where <span class="iml">$\bigtriangleup.[\odot]:=\bigtriangleup.\mathrm{map}(x\to x.\odot)$</span>.</p>
			
			<p>Let <span class="iml">$n \in \mathrm{N}$</span>, <span class="imb">$\dot{a}^2 \in \ddot{a}^2 \cap \mathrm{E[n]}$</span> and <span class="imb">$\dot{b}^1 \in \ddot{b}^1 \cap \mathrm{E[n]}$</span> where <span class="imb">$\mathrm{E[n]}$</span> are events that happened on the $n$ node.</p>

			<p><span class="imr">$\dot{a}^2.\mathrm{ts} &lt; \dot{b}^1.\mathrm{ts}$</span> holds because acceptor doesn't accept states with lower ballot number when it promised to accept a state with higher ballot number and <span class="iml">$a&lt;b$</span>.</p>
			
			<p>By definition <span class="imb">$\dot{a}^2.\mathrm{accepted\_n}$</span> is the ballot number of the accepted state at moment <span class="imb">$\dot{a}.\mathrm{ts}$</span> on node $n$. The same is also true for $\dot{b}^1$.</p>

			<p>Since the ballot numbers of the accepted state is a monotonically increasing function of time then</p>

			$$\dot{a}^2.\mathrm{accepted\_n} \;\leq\; \dot{b}^1.\mathrm{accepted\_n}$$

			<p>By definition <span class="imb">$\dot{b}^1.\mathrm{accepted\_n} \in \bar{b}^1.\mathrm{reads}.[\mathrm{accepted\_n}]$</span> so</p>

			$$\dot{b}^1.\mathrm{accepted\_n} \;\leq\; \bar{b}^1.\mathrm{reads}.\mathrm{max}(x \to x.\mathrm{accepted\_n}).\mathrm{accepted\_n} \;=\; \mathrm{unwrap}(\ddot{b}^2)$$

			<p>And it's the final in proving the lemma since:</p>

			$$a \;=\; \dot{a}^2.\mathrm{accepted\_n} \;\leq\; \dot{b}^1.\mathrm{accepted\_n} \;\leq\; \mathrm{unwrap}(\ddot{b}^2)$$

			<div class="qed">Q.E.D.</div>
		</div>
</div></div>

<div id="conclusion1" class="brim"><div class="content">
	<h2>Conclusion</h2>
</div></div>

<div class="brim"><div class="content">
	<p class="edge">The family of Paxos algorithms provides powerful primitives for building robust distributed systems and data structures like distributed switch, distributed variable (a foundation for a hashtable aka key/value storage), distributed log and a generic state machine.</p>
</div></div>

<div class="brim alt examples1"><div class="content">
	<p class="edge">For example we'll define a distributed switch and variable as a simple layer on the algorithm described in this post.</p>

	<h3>Distributed switch</h3>

	{% gist rystsov/0f5f6bcb82b9d666769d %}

	<h3>Distributed variable</h3>

	{% gist rystsov/23ff19e75903f6d87b39 %}
</div></div>

<div class="brim"><div class="content">
	<p class="edge">If you read <a class="link" href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">Paxos Made Simple</a> then you might have noticed that the algorithm in this post is a little bit different. Leslie Lamport described a distributed switch (aka single degree Paxos) and shown how to run multiple copies of it to build a distributed state machine (Multi-Paxos).</p>

	<blockquote>&gt; It seems that the "Paxos Made Simple" version is more complex. Is it really true?</blockquote>

	<p>Yes it is, but it doesn't mean that it is worse; it just does more and it is natural to expect that it is more complex. For example Multi-Paxos:</p>

	<ol>
		<li>supports several concurrent state modifications if they don't conflict (see the $\alpha$ parameter)</li>
		<li>keeps the history of all commands</li>
	</ol>

	<blockquote>&gt; Are the Multi-Paxos's extra features important?</blockquote>

	<p>It depends. If we want to build a key/value storage then we can take Multi-Paxos or Raft and put the key/value logic behind it. In theory performance of the storage should be limited by network and disk throughput because the operations with different keys are independent and the algorithm supports concurrency. But in practice performance of the well known CP systems is low.</p>

	<p>According to the official documents<sup><a class="link" href="http://wiki.apache.org/hadoop/ZooKeeper/Performance">[1]</a></sup> performance of the 3-node Zookeeper cluster is close to 20K ops per second. Etcd is another famous CP system and its performance is close to 5K ops per second<sup><a class="link" href="https://github.com/coreos/etcd/blob/master/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md">[2]</a></sup>.</p>

	<p>However average hardware can provide 150Mb/s for HDD, so the expected performance should be close to 200K ops/s.</p>

	<p>It happens because Raft and Multi-Paxos has intrinsic limitation on their concurrency support. For example when Multi-Paxos inserts a value into the log it should invalidate all previous unknown positions in the log. The number of unknown positions depends on the same $\alpha$ parameter that controls the number of concurrent operations, so it's obvious that the performance as a function of $\alpha$ looks like an upside down parabola and achieves its maximum.</p>

	<p>If we take another approach and run an independent instance of single degree Paxos presented in this post per each key then we can achieve much better performance because we don't coordinate the updates to different keys. This idea aligns both with academia and industry. For example, distributed systems researcher Peter Bailis made the <a class="link" href="https://www.youtube.com/watch?v=EYJnWttrC9k">Coordination-Avoiding Systems Design</a> talk about the similar idea. Folks behind the CockroachDB <a class="link" href="https://github.com/cockroachdb/cockroach/blob/master/docs/design.md">chose a transitional approach</a> for their db: they break the key space into a set of small ranges and run independent Raft implementation per each range.</p>

	<p>So Multi-Paxos and Raft have advantage over single degree Paxos state machine, but for some applications (like key/value storages) those advantage are irrelevant and introduce redundant complexity.</p>
</div></div>
